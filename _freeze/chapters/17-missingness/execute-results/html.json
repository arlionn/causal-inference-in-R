{
  "hash": "9ffa12970c2fbdedb2919f5691699948",
  "result": {
    "markdown": "# Missingness {#sec-missingness}\n\n\n\n\n\nMissing data is a problem for most real datasets that impacts all three types of analysis: description, prediction, and causal inference. As with analysis, the impact of missingness is also different across all three, even when we are using the same tools (like multiple imputation) to address it. You can think of missingness as an extreme version of measurement error: we intended to measure certain variables that are important for the analysis and in some capacity have not succeeded. In this chapter, we'll explore how bias can arise in causal analyses in the presence of missingness and how to address them (if we can!).\n\n## Missingness and measurement error as structural bias\n\nIt's frequently been observed that causal inference can be thought of fundamentally as a missing data problem; after all, what we really want to compare is counterfactual states, but all such states outside of the factual world are missing. Causal inference as missingness is interesting philosophically and connects methods from the two areas of statistics. Here, we'll consider the reverse proposition: missingness (and measurement error) is a causal inference problem.\n\nThus far, we've made a big assumption in our DAGs that the variables that we've included are actually the variables we have in our data. In other words, we assume that the data is measured perfectly. This is almost always untrue. Let's consider a few scenarios using causal diagrams to understand the impact of mismeasurement and missingness.\n\nBoth measurement error and missingness are often visualized in DAGs as either an *indicator* of missingness, something akin to second variable on which you're conditioning, and a second variable that represents the variable as you have it measured. We'll use the second approach: Each variable we'll have a second node that represents the variable-as-observed. Let's look at an example.\n\nIn the Touring Plans data, most variables are complete and likely measured with high accuracy (e.g., ticket season and historic weather are well-measured). One variable with missingness and measurement error is the actual wait times for rides. If you recall, the data relies on humans to wait in line, either consumers of the data who report their experience or people hired to wait in line to report the wait time. Thus, missingness is largely related to whether or not someone is there there to measure the wait time. When someone is there to measure it, it's also likely measured with error, and that error likely depends on who the person is, e.g. someone estimating their wait time and submitting it to Touring Plans is likely producing a value with more error than someone paid to stay in line and count the minutes. Both aspects can be captured by treating the measured value as a separate node. \n\nThat said, let's take measurement error and missingness one at a time. First, let's consider measurement error. In @fig-meas-err-dag, the measured versions of actual and posted wait times have two variables that influence them: the real values and unknown or unmeasured factors that influence their mismeasurement. Notably, the ways in which the two wait times are mismeasured are independent of one another. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](17-missingness_files/figure-html/fig-meas-err-dag-1.png){#fig-meas-err-dag width=672}\n:::\n:::\n\n\nThe posted wait times are scraped by Touring Plans from what Disney posts on the web. There is a plausible mechanism by which which this could be mismeasured: Disney could post the wrong times online compared to what's posted at the park, or Touring Plans could have a mistake in its data acquisition code. That said, it's reasonable to assume that this error would probably be small.\n\nActual wait times, on the other hand, probably have a good deal of measurement error. Humans have to manually measure this period, so there is some natural error in that process. Measurement also likely depends on whether or not the person measuring the wait time is being paid to do so. Presumably, someone who is entering it out of good will is more likely to submit an approximate wait time that they experienced, and someone who is paid is more likely to be precise. \n\nSo, an update DAG might look like @fig-meas-err-other-1. This DAG describes the structural causes of measurement error in `actual`; notably, though, that structure is not connected to `posted`. This is easier to see if we show a *null* DAG in which there is no arrow from `posted` to `actual` (@fig-meas-err-other-2). Posted time is not connected to the mechanisms that cause measurement error in actual[^1]. In other words, the bias caused by this measurement error is *not* due to nonexchangability; it's due to a numerical error in the measurement, not an open path in the causal graph.\n\n<!-- TODO: cross check independent vs differential -->\nInstead of the type of nonexchangability that is induced by confounding, this type of bias in measurement is due to the fact that we're analyzing the effect of `posted` to `actual_measured` as a proxy for the effect of `posted` on `actual`. The correctness of the result depends entirely on how well `actual_measured` approximates `actual`. This is called *non-differential* measurement error: the error is not due to structural nonexchangability but . \n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](17-missingness_files/figure-html/fig-meas-err-other-1.png){#fig-meas-err-other-1 width=672}\n:::\n\n::: {.cell-output-display}\n![](17-missingness_files/figure-html/fig-meas-err-other-2.png){#fig-meas-err-other-2 width=672}\n:::\n:::\n\n\nLet's say, however, that there is another unknown factor that influences both the posted wait time and the measurement of the actual wait time.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](17-missingness_files/figure-html/fig-meas-err-dag-diff-1.png){#fig-meas-err-dag-diff width=672}\n:::\n:::\n\n\n\nIn these DAGs, we assumed that the measurement of actual wait time has no impact on posted wait time and vice versa. Because posted time happens before actual time, it's reasonable to assume that that is the case for that relationship. However, the posted wait time *may* influence the *missingness* of the actual wait time. If the posted wait time is high, for instance, someone may not get in the line at all. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](17-missingness_files/figure-html/fig-missing-dag-1.png){#fig-missing-dag width=672}\n:::\n:::\n\n\nIn addition, the confounders in the causal model may also contribute to the missingness of actual wait time, say if season or temperature influences whether TouringPlans sends someone in to do a measurement.\n\n## Multiple Imputation\n\n## Combining MICE and IPW\n",
    "supporting": [
      "17-missingness_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}